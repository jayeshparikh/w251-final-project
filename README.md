# Lip Reading using Computer Vision
<p>This repository contains code for <b>W251 Final Project - Watch The Whisper</b>, an intersection of Speech, Computer Vision and Natural Language Processing. This code is based on <a href="https://github.com/LordMartian/deep_avsr" rel="nofollow">Deep Audio-Visual Speech Recognition</a>, which is a PyTorch reproduction of the TM-CTC model from the <a href="https://arxiv.org/abs/1809.02108" rel="nofollow">Deep Audio-Visual Speech Recognition</a> paper.</p>

<p>The model was trained on <a href="http://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html" rel="nofollow">LRS2 dataset</a> for the speech-to-text transcription task</p>

<h3>Requirements</h3>
Recommended way to install the dependencies is creating a new virtual environment and then running <i>requirements.txt</i> file under server/src

<code>pip install -r requirements.txt</code>

